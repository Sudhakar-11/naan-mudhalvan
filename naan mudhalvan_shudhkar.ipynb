{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 5,
  "nbformat": 4,
  "cells": [
    {
      "id": "d02ff1a9-6af3-4da0-9def-5f966ce46427",
      "cell_type": "code",
      "source": "import numpy as np\nimport random\n\n# Define environment parameters\nNUM_STATES = 10  # Simplified state space (e.g., vehicle queue level: 0-9)\nNUM_ACTIONS = 2  # 0 = NS Green, 1 = EW Green\nEPISODES = 5000\n\nALPHA = 0.1      # Learning rate\nGAMMA = 0.9      # Discount factor\nEPSILON = 0.1    # Exploration rate\n\n# Initialize Q-table\nQ = np.zeros((NUM_STATES, NUM_ACTIONS))\n\n# Simulated traffic environment\ndef get_next_state(state, action):\n    if action == 0:\n        # NS green - reduce NS queue, increase EW\n        ns = max(0, state - random.randint(1, 3))\n        ew = min(NUM_STATES - 1, random.randint(0, 3))\n    else:\n        # EW green - reduce EW queue, increase NS\n        ew = max(0, state - random.randint(1, 3))\n        ns = min(NUM_STATES - 1, random.randint(0, 3))\n    return (ns + ew) // 2  # simplified next state (average queue level)\n\ndef get_reward(state, action):\n    # Lower state value means shorter queue, which is better\n    return -state  # reward is negative queue length\n\n# Q-learning algorithm\nfor episode in range(EPISODES):\n    state = random.randint(0, NUM_STATES - 1)\n    for t in range(100):  # time steps per episode\n        # Choose action (epsilon-greedy)\n        if random.random() < EPSILON:\n            action = random.randint(0, NUM_ACTIONS - 1)\n        else:\n            action = np.argmax(Q[state])\n        \n        next_state = get_next_state(state, action)\n        reward = get_reward(state, action)\n\n        # Update Q-value\n        Q[state][action] += ALPHA * (reward + GAMMA * np.max(Q[next_state]) - Q[state][action])\n        state = next_state\n\n# Test policy\nprint(\"Trained Q-table:\")\nprint(Q)\n\ndef get_optimal_action(queue_level):\n    return \"NS Green\" if np.argmax(Q[queue_level]) == 0 else \"EW Green\"\n\n# Example decision\ncurrent_queue = 6\nprint(f\"Current queue level: {current_queue}, Suggested signal: {get_optimal_action(current_queue)}\")\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Trained Q-table:\n[[ -4.70698225  -4.38575038]\n [ -5.18443789  -5.73467473]\n [ -6.75251012  -6.89085226]\n [ -7.9291886   -8.08184988]\n [ -9.39063566  -9.67148279]\n [-10.76453974 -11.20754648]\n [-12.97467275 -12.43991618]\n [-14.06730281 -14.52705293]\n [-15.68682311 -16.14510701]\n [-17.82727652 -17.39628473]]\nCurrent queue level: 6, Suggested signal: EW Green\n"
        }
      ],
      "execution_count": 1
    },
    {
      "id": "0ee67d3c-5321-49fb-889c-829dfe67907f",
      "cell_type": "code",
      "source": "",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}